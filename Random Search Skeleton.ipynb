{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffdd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Clustering import Clustering\n",
    "from DataNoiseAdder import DataNoiseAdder\n",
    "from DatasetCorruptor import DatasetCorruptor\n",
    "from DecisionTreeEnsemble import DecisionTreeEnsemble\n",
    "from SyntheticDataGenerator import SyntheticDataGenerator\n",
    "from EnsembleDiversity import EnsembleDiversity\n",
    "from EnsembleMetrics import EnsembleMetrics\n",
    "\n",
    "from utils import get_dataset, get_ensemble_preds_from_models, get_precision_recall_auc, auprc_threshs\n",
    "from utils import plot_precision_recall, plot_aroc_at_curve, fitness_scatter\n",
    "from utils import compute_metrics_in_buckets, flatten_df, compute_cluster_metrics\n",
    "from utils import get_categorical_and_float_features\n",
    "from utils import get_clusters_dict, make_noise_preds\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258423e4",
   "metadata": {},
   "source": [
    "### Experiment Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f307604",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['ntrls'] = 10\n",
    "args['ensemble_size'] = 10\n",
    "args['dataset_path'] = \"/Users/scottmerrill/Documents/UNC/Research/OOD-Ensembles/datasets\"\n",
    "args['dataset_name'] = 'heloc_tf' \n",
    "\n",
    "# Decision Tree/Model Pool Params\n",
    "args['num_classifiers'] = 100\n",
    "args['feature_fraction'] = 0.5\n",
    "args['data_fraction'] = 0.8\n",
    "args['max_depth'] = 10\n",
    "args['min_samples_leaf'] = 5\n",
    "args['random_state'] = 1\n",
    "args['clusters_list'] = [3, 10]\n",
    "args['shift_feature_count'] = 5\n",
    "\n",
    "AUCTHRESHS = np.array([0.1, 0.2, 0.3, 0.4, 1. ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce87de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val_id, y_val_id, x_val_ood, y_val_ood = get_dataset(args['dataset_path'] , args['dataset_name'])\n",
    "num_features = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33f186",
   "metadata": {},
   "source": [
    "### Building and Training Model Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b22ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pool = DecisionTreeEnsemble(args['num_classifiers'], \n",
    "                                  args['feature_fraction'],\n",
    "                                  args['data_fraction'],\n",
    "                                  args['max_depth'],\n",
    "                                  args['min_samples_leaf'],\n",
    "                                  args['random_state'])\n",
    "\n",
    "# train model pool\n",
    "model_pool.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00837998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble saved to test.pkl\n",
      "Ensemble loaded from test.pkl\n"
     ]
    }
   ],
   "source": [
    "    def save(self, file_path):\n",
    "        \"\"\"\n",
    "        Save the entire state of the ensemble to a file.\n",
    "        \n",
    "        :param file_path: The path where the model will be saved.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'num_classifiers': self.num_classifiers,\n",
    "                'feature_fraction': self.feature_fraction,\n",
    "                'data_fraction': self.data_fraction,\n",
    "                'max_depth': self.max_depth,\n",
    "                'min_samples_leaf': self.min_samples_leaf,\n",
    "                'random_state': self.random_state,\n",
    "                'classifiers': self.classifiers,\n",
    "                'feature_subsets': self.feature_subsets,\n",
    "                'data_subsets': self.data_subsets\n",
    "            }, f)\n",
    "        print(f\"Ensemble saved to {file_path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, file_path):\n",
    "        \"\"\"\n",
    "        Load the ensemble from a saved file.\n",
    "        \n",
    "        :param file_path: The path to the saved ensemble file.\n",
    "        :return: An instance of DecisionTreeEnsemble with the loaded state.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Create an instance of the class\n",
    "        ensemble = cls(\n",
    "            num_classifiers=data['num_classifiers'],\n",
    "            feature_fraction=data['feature_fraction'],\n",
    "            data_fraction=data['data_fraction'],\n",
    "            max_depth=data['max_depth'],\n",
    "            min_samples_leaf=data['min_samples_leaf'],\n",
    "            random_state=data['random_state']\n",
    "        )\n",
    "        \n",
    "        # Restore the saved classifiers, feature, and data subsets\n",
    "        ensemble.classifiers = data['classifiers']\n",
    "        ensemble.feature_subsets = data['feature_subsets']\n",
    "        ensemble.data_subsets = data['data_subsets']\n",
    "        \n",
    "        print(f\"Ensemble loaded from {file_path}\")\n",
    "        \n",
    "        return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f60bfaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_pool.predict(x_val_id) == model_pool2.predict(x_val_id)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1084d",
   "metadata": {},
   "source": [
    "### Caching Model Pool Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bff972",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pool_preds = model_pool.predict(x_val_ood)\n",
    "model_pool_pred_probs = model_pool.predict_proba(x_val_ood)\n",
    "mp_precision, mp_recall, mp_auc = get_precision_recall_auc(model_pool_pred_probs, y_val_ood, AUCTHRESHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42afc68",
   "metadata": {},
   "source": [
    "### Caching Individual Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pool.train_preds = model_pool.get_individual_predictions(x_train).T\n",
    "model_pool.train_pred_probs = model_pool.get_individual_probabilities(x_train)\n",
    "\n",
    "model_pool.val_id_preds = model_pool.get_individual_predictions(x_val_id).T\n",
    "model_pool.val_id_pred_probs = model_pool.get_individual_probabilities(x_val_id)\n",
    "\n",
    "model_pool.val_ood_preds = model_pool.get_individual_predictions(x_val_ood).T\n",
    "model_pool.val_ood_pred_probs = model_pool.get_individual_probabilities(x_val_ood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0f4d6",
   "metadata": {},
   "source": [
    "### Clustering Data and noise methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0effc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clusters_dict = {}\n",
    "all_clusters_dict['train_preds'], all_clusters_dict['val_id']  = get_clusters_dict(x_train, x_val_id, args['clusters_list'])\n",
    "all_clusters_dict = make_noise_preds(x_train, y_train, x_val_id, model_pool, args['shift_feature_count'], args['clusters_list'], all_clusters_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe31641",
   "metadata": {},
   "source": [
    "### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = SyntheticDataGenerator(x_train, y_train)\n",
    "\n",
    "# interpolation\n",
    "interp_x, interp_y = generator.interpolate(x_train.shape[0])\n",
    "model_pool.synth_interp_preds = model_pool.get_individual_predictions(interp_x).T\n",
    "model_pool.synth_interp_pred_probs = model_pool.get_individual_probabilities(interp_x)\n",
    "del interp_x\n",
    "\n",
    "# GMM\n",
    "gmm_x, gmm_y = generator.gaussian_mixture(x_train.shape[0])\n",
    "model_pool.synth_gmm_preds = model_pool.get_individual_predictions(gmm_x).T\n",
    "model_pool.synth_gmm_pred_probs = model_pool.get_individual_probabilities(gmm_x)\n",
    "del gmm_x\n",
    "\n",
    "dt_x, dt_y = generator.decision_tree(x_train.shape[0])\n",
    "model_pool.synth_dt_preds = model_pool.get_individual_predictions(dt_x).T\n",
    "model_pool.synth_dt_pred_probs = model_pool.get_individual_probabilities(dt_x)\n",
    "del dt_x\n",
    "\n",
    "synth_data_dict = {'synth_interp':interp_y.astype('int64'),\n",
    "                   'synth_gmm': gmm_y.astype('int64'),\n",
    "                   'synth_dt': dt_y.astype('int64')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319b260",
   "metadata": {},
   "source": [
    "### Label Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155295d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_flipped = DataNoiseAdder.label_flip(y_train)\n",
    "y_val_flipped = DataNoiseAdder.label_flip(y_val_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ca5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1ff75",
   "metadata": {},
   "source": [
    "### Random Search Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6634a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [x for x in dir(model_pool) if 'preds' in x]\n",
    "b = [x for x in dir(model_pool) if 'pred_' in x]\n",
    "\n",
    "a.remove(f'val_ood_preds')\n",
    "b.remove(f'val_ood_pred_probs')\n",
    "\n",
    "pred_attributes = [(a[i], b[i], a[i].split('_')[0]+'_'+a[i].split('_')[1]) for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_df = pd.DataFrame()\n",
    "recalls_df = pd.DataFrame()\n",
    "aucs_df = pd.DataFrame()\n",
    "fitness_df = pd.DataFrame()\n",
    "\n",
    "for trial in tqdm(range(args['ntrls'])):\n",
    "    indices = np.random.choice(model_pool.num_classifiers, size=args['ensemble_size'], replace=True)\n",
    "\n",
    "    # ood preds of sub-ensemble\n",
    "    ood_preds, ood_pred_probs = get_ensemble_preds_from_models(model_pool.val_ood_pred_probs[indices])\n",
    "\n",
    "    # save OOD precision/recalls seprately\n",
    "    precision, recall, auc = get_precision_recall_auc(ood_pred_probs, y_val_ood, AUCTHRESHS)\n",
    "\n",
    "    recalls_df = pd.concat([recalls_df, pd.DataFrame(recall)], axis=1)\n",
    "    precisions_df = pd.concat([precisions_df, pd.DataFrame(precision)], axis=1)\n",
    "    aucs_df = pd.concat([aucs_df, pd.DataFrame(auc)], axis=1)\n",
    "\n",
    "    tmp = {'generation':trial,\n",
    "              'ensemble_files':','.join(str(x) for x in indices)}\n",
    "    cluster_metrics = pd.DataFrame()\n",
    "\n",
    "    # Compute all Fitness Metrics\n",
    "    for label_flip in [0, 1]:\n",
    "        for pred_tuple in pred_attributes:\n",
    "            preds_name, pred_prob_name, prefix_name = pred_tuple\n",
    "\n",
    "            # get clustering associated with data transformation\n",
    "            if 'synth' not in prefix_name:\n",
    "                clusters_dict = all_clusters_dict[prefix_name]\n",
    "                \n",
    "            if 'train' in prefix_name:\n",
    "                if label_flip:\n",
    "                    Y = y_train_flipped\n",
    "                    prefix_name = prefix_name + '_flip'\n",
    "                else:\n",
    "                    Y = y_train\n",
    "                    \n",
    "            elif 'synth' in prefix_name:\n",
    "                if label_flip:\n",
    "                    continue\n",
    "                else:\n",
    "                    Y = synth_data_dict[prefix_name]\n",
    "\n",
    "            else:\n",
    "                if label_flip:\n",
    "                    prefix_name = prefix_name + '_flip'\n",
    "                    Y = y_val_flipped\n",
    "                else:\n",
    "                    Y = y_val_id\n",
    "\n",
    "            model_pool_preds = getattr(model_pool, preds_name)\n",
    "            model_pool_pred_probs = getattr(model_pool, pred_prob_name)\n",
    "\n",
    "            model_preds = model_pool_preds[indices]\n",
    "            model_pred_probs = model_pool_pred_probs[indices]\n",
    "\n",
    "            # id val preds of sub-ensemble\n",
    "            ensemble_preds, ensemble_pred_probs = get_ensemble_preds_from_models(model_pred_probs)\n",
    "            metrics = EnsembleMetrics(Y, ensemble_preds, ensemble_pred_probs[:,1])\n",
    "            diversity = EnsembleDiversity(Y, model_preds)\n",
    "\n",
    "            tmp.update({f'{prefix_name}_acc':metrics.accuracy(),\n",
    "                   f'{prefix_name}_auc':metrics.auc(),\n",
    "                   f'{prefix_name}_prec':metrics.precision(),\n",
    "                   f'{prefix_name}_rec':metrics.recall(),\n",
    "                   f'{prefix_name}_f1':metrics.f1(),\n",
    "                   f'{prefix_name}_mae':metrics.mean_absolute_error(),\n",
    "                   f'{prefix_name}_mse':metrics.mean_squared_error(),\n",
    "                   f'{prefix_name}_logloss':metrics.log_loss(),\n",
    "\n",
    "                   # diversity\n",
    "                   f'{prefix_name}_q_statistic':np.mean(diversity.q_statistic()),\n",
    "                   f'{prefix_name}_correlation_coefficient':np.mean(diversity.correlation_coefficient()),\n",
    "                   f'{prefix_name}_entropy':np.mean(diversity.entropy()),\n",
    "                   f'{prefix_name}_diversity_measure':diversity.diversity_measure(),\n",
    "                   f'{prefix_name}_hamming_distance':np.mean(diversity.hamming_distance()),\n",
    "                   f'{prefix_name}_error_rate':np.mean(diversity.error_rate()),\n",
    "                   f'{prefix_name}_auc':np.mean(diversity.auc()),\n",
    "                   f'{prefix_name}_brier_score':np.mean(diversity.brier_score()),\n",
    "                   f'{prefix_name}_ensemble_variance':np.mean(diversity.ensemble_variance()),\n",
    "                  })\n",
    "\n",
    "            if 'synth' not in prefix_name:\n",
    "                # compute cluster metrics\n",
    "                tmp_cluster = compute_cluster_metrics(clusters_dict, ensemble_preds, model_preds, model_pred_probs, Y)\n",
    "                col_names = [prefix_name + '_' + x for x in tmp_cluster.columns]\n",
    "                col_names = [name.replace('_val_acc', '') for name in col_names]\n",
    "                col_names = [name.replace('_train_acc', '') for name in col_names]\n",
    "                tmp_cluster.columns = col_names\n",
    "\n",
    "                cluster_metrics = pd.concat([cluster_metrics, tmp_cluster], axis=1)\n",
    "\n",
    "    raw_metrics = pd.DataFrame([tmp])    \n",
    "    tmp = pd.concat([raw_metrics, cluster_metrics], axis=1)\n",
    "    fitness_df = pd.concat([fitness_df, tmp])\n",
    "    #precisions_df.to_csv(save_path+'/precisions_df.csv', index=False)\n",
    "    #recalls_df.to_csv(save_path+'/recalls_df.csv', index=False)\n",
    "    #aucs_df.to_csv(save_path+'/aucs_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "fitness_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa059c42",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df = fitness_df.reset_index(drop=True)\n",
    "best_fitness_index = {i+1:index for i,index in enumerate(fitness_df.nlargest(3, 'val_upscaleshift_meanshift_val_error_rate_mean').index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b386f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "plot_precision_recall(precisions_df, recalls_df, mp_precision, mp_recall, best_fitness_index, ax=axs[0])\n",
    "plot_aroc_at_curve(AUCTHRESHS, aucs_df, mp_auc, best_fitness_index, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380ba38",
   "metadata": {},
   "source": [
    "### Fitness Function Diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['train_boundaryshift_acc', 'val_upscaleshift_meanshift_val_error_rate_mean']\n",
    "fig, axs = plt.subplots(1, len(cols), figsize=(12, 6))\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    # Plot AUC scatter for different fitness columns\n",
    "    fitness_scatter(fitness_df, aucs_df, col, ax=axs[idx])\n",
    "plt.tight_layout(pad=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bee31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
